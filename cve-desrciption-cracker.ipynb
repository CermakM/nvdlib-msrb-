{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "\n",
    "from nvdlib.nvd import NVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "df = pd.read_csv('dataframe-nvd-2017.csv', converters={'version_range': ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": true,
        "row": 4,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "\n",
    "from plotly import graph_objs as go\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 2,
        "hidden": false,
        "row": 0,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "\n",
    "The data consists of CPEs with direct reference to GitHub. This way it is possible to label the data with the project name infered from GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "nvd = NVD.from_feeds(['2017'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "nvd.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "GH_BASE_URL = u\"http[s]://github.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ecos = ['Java', 'JavaScript', 'Python']\n",
    "df_ecos = df[ecos]\n",
    "df_ecos = df[['username', 'project', 'version_range', 'url', *ecos]][df_ecos.any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def get_reference(cve, url=None, pattern=None) -> str:\n",
    "    for ref in cve.references:\n",
    "        if url and url == ref:\n",
    "            return ref\n",
    "\n",
    "        if re.search(pattern, ref):\n",
    "            return ref\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def strip_src_url(url: str):\n",
    "    \"\"\"ATM assume that the only reference to source is github.\"\"\"\n",
    "    url_base_pattern = u\"http[s]://github.com/([\\w-]+)/([\\w-]+[.]*[\\w-]+)\"\n",
    "    strip_url = re.search(url_base_pattern, url)\n",
    "    \n",
    "    if not strip_url:\n",
    "        print(url)\n",
    "        return None\n",
    "    \n",
    "    return strip_url[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Get descriptions and append them to the current DataFrame to avoid recreating a new one from scratch\n",
    "cves = dict()\n",
    "cve_list = list()\n",
    "\n",
    "for cve in nvd.cves():\n",
    "    ref = get_reference(cve, pattern=\"http[s]://github.com\")\n",
    "    if ref is None:\n",
    "        continue\n",
    "        \n",
    "    ref = strip_src_url(ref)\n",
    "    cve_list.append((cve.cve_id, ref, cve.description))\n",
    "    cves[cve.cve_id] = cve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_desc = pd.DataFrame(cve_list, columns=['cve_id', 'url', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df_ecos, df_desc, how='inner', on='url').set_index(['username', 'project'])\n",
    "del df_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()\n",
    "df = df[[\n",
    "    'cve_id',\n",
    "    'url',\n",
    "    'description',\n",
    "    'version_range',\n",
    "    'Java',\n",
    "    'JavaScript',\n",
    "    'Python',\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 2,
        "hidden": false,
        "row": 2,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "Sample of the initial unlabeled data for the 3 ecosystems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 13,
        "hidden": false,
        "row": 4,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "df.sample(frac=0.1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 17,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### Create toy data set\n",
    "\n",
    "For the purpose of quicker evaluation of feature extractos and classification accuracy, toy data set will be created.\n",
    "\n",
    "A small portion (about $10%$) of random samples will be taken from the dataset filtered on the Java ecosystem (when Java works... the rest of it will, too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "toy_df = df[['cve_id', 'description']][df.Java > 1E2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# label_tuple are a position of the project token in the token list\n",
    "label_tuple = [None] * len(toy_df)\n",
    "for i, (index, row) in enumerate(toy_df.iterrows()):\n",
    "    proj = index[1].lower()\n",
    "    desc = row.description.lower()\n",
    "    # find the position of proj in the description, if applicable\n",
    "    tokens = nltk.word_tokenize(desc)\n",
    "    found = False\n",
    "    for pos, token in enumerate(tokens):\n",
    "        if token == proj:\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        pos = None\n",
    "    label_tuple[i] = (row.cve_id, token, pos)\n",
    "\n",
    "# turn index into series\n",
    "label_series = pd.DataFrame(label_tuple, columns=['cve_id', 'label', 'pos'])\n",
    "\n",
    "del label_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "toy_df = toy_df.reset_index().merge(label_series, how='outer', on='cve_id').set_index(['username', 'project'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# remove the projects where no project name was found\n",
    "toy_df = toy_df[toy_df.pos.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 2,
        "hidden": false,
        "row": 21,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "Sample of the labeled toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 8,
        "hidden": false,
        "row": 23,
        "width": 12
       },
       "report_default": {}
      }
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toy_df.sample(frac=0.5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 5,
        "hidden": false,
        "row": 31,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Classificator training and evaluation\n",
    "\n",
    "Model chosen for this approach will make use of Naive Bayes classification.\n",
    "\n",
    "Since it is unknown whether the current data evince a latent pattern, multiple feature extractors will be suggested and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def similar(word, cpes) -> bool:\n",
    "Since it is unknown whether the current data evince a latent pattern, multiple\n",
    "    for cpe in cpes:\n",
    "        vendor, = cpe.vendor\n",
    "        product, = cpe.product\n",
    "#         if word.lower() in {vendor.lower(), product.lower()}:\n",
    "        if product.lower().find(word.lower()) != -1:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def create_feature_list_long(feature_extractor, sents, labels, cve_ids) -> list:\n",
    "    \"\"\"Uses all sentences to create feature list given feature extractor.\"\"\"\n",
    "    feature_list = list()\n",
    "    for i, desc in enumerate(sents):\n",
    "        label = labels[i]\n",
    "        tagged_sent = nltk.pos_tag(nltk.word_tokenize(desc), tagset='universal')\n",
    "        for j, (word, tag) in enumerate(tagged_sent):\n",
    "            is_label = word.lower() == label.lower()\n",
    "            features = feature_extractor(tagged_sent, j, cve_ids[i])\n",
    "            feature_list.append((features, is_label))\n",
    "    \n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def eval_accuracy(classifier, extractor, sentences: list, labels: list,\n",
    "                  cve_ids: list, n=1, verbose=False) -> float:\n",
    "    \"\"\"Evaluate accuracy using raw classificator output.\"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    assert len(sentences) == len(labels)\n",
    "    \n",
    "    accurates = [None] * len(labels)\n",
    "    for i, sent in enumerate(sentences):\n",
    "        tagged = nltk.pos_tag(nltk.word_tokenize(sent), tagset='universal')\n",
    "        prob_dist = [classifier.prob_classify(extractor(tagged, j, cve_ids[i])) for j in range(len(tagged))]\n",
    "        probs = [(word, tag, prob.prob(True)) for (word, tag), prob in zip(tagged, prob_dist)]\n",
    "        probs  = sorted(probs, key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        most_prob = set([prob[0].lower() for prob in probs[:n]])\n",
    "        accurates[i] = labels[i].lower() in most_prob\n",
    "    \n",
    "        if verbose:\n",
    "            print('Sentence: ', sent)\n",
    "            print('Expected: `%s`' % labels[i], 'got: `%s`' % most_prob, '\\n')\n",
    "    \n",
    "    bag = Counter(accurates)\n",
    "    return bag[True] / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "blacklist = set(nltk.corpus.stopwords.words())\n",
    "blacklist.update(set(['kernel', 'function', 'version', 'functions', 'versions', '<', '=', '.', '>']))\n",
    "\n",
    "def predict(sent, classifier, extractor, cve_id, n=1, verbose=False) -> list:\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(sent), tagset='universal')\n",
    "    \n",
    "    prob_dist = [classifier.prob_classify(extractor(tagged, j, cve_id)) for j in range(len(tagged))]\n",
    "    probs = set([(word.lower(), prob.prob(True)) for (word, tag), prob in zip(tagged, prob_dist)\n",
    "                 if word.lower() not in blacklist and tag != 'NUM'\n",
    "                ])\n",
    "    \n",
    "    probs = sorted(probs, key=lambda x: x[1], reverse=True)\n",
    "    if verbose:\n",
    "        print(probs)\n",
    "    return probs[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def eval_accuracy_spec(classifier, extractor, sentences: list, labels: list, cve_ids: list,\n",
    "                       n=1, verbose=False) -> float:\n",
    "    \"\"\"Evaluate accuracy using predict function.\n",
    "    This also filters out blacklisted words and stopwords.\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    assert len(sentences) == len(labels)\n",
    "    \n",
    "    accurates = [None] * len(labels)\n",
    "    for i, sent in enumerate(sentences):\n",
    "        probs = predict(sent, classifier, extractor, cve_ids[i], n)\n",
    "        \n",
    "        most_prob = set([p[0].lower() for p in probs])\n",
    "        \n",
    "        accurates[i] = False\n",
    "        for prob in most_prob:\n",
    "            if labels[i].lower().find(prob) != -1:\n",
    "                accurates[i] = True\n",
    "    \n",
    "        if not accurates[i] and verbose:\n",
    "            print('Sentence: ', sent)\n",
    "            print('Expected: `%s`' % labels[i], 'got: `%s`' % most_prob, '\\n')\n",
    "    \n",
    "    bag = Counter(accurates)\n",
    "    return bag[True] / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def extract_features_vanilla(tagged: list, pos: int, cve_id=None):\n",
    "    \"\"\"Extract contextual features from the sentence w.r.t given position of a word.\"\"\"\n",
    "    word, tag = tagged[pos]\n",
    "    features = {\n",
    "        'tag': tag,\n",
    "        'has-uppercase': word[0].isupper(),\n",
    "        'word-len': len(word) > 3,\n",
    "    }\n",
    "    if pos == 0:\n",
    "        features['prev-tag'] = '<start>'\n",
    "    else:\n",
    "        features['prev-word'] = tagged[pos - 1][0].lower()\n",
    "        features['prev-tag'] = tagged[pos - 1][1]\n",
    "        \n",
    "    return features\n",
    "\n",
    "# # Lets not split the dataset here for now\n",
    "# feature_list = create_feature_list_long(extract_features_vanilla, descriptions, labels, cve_ids)\n",
    "\n",
    "# classifier = nltk.NaiveBayesClassifier.train(feature_list)\n",
    "# classifier.show_most_informative_features()\n",
    "\n",
    "# eval_accuracy(classifier, extract_features_vanilla, descriptions, labels, cve_ids, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_features_v0(tagged: list, pos: int, cve_id=None):\n",
    "    \"\"\"Extract contextual features from the sentence w.r.t given position of a word.\"\"\"\n",
    "    word, tag = tagged[pos]\n",
    "    features = {\n",
    "        'tag': tag,\n",
    "        'word-len': len(word) > 3,\n",
    "    }\n",
    "    if pos == 0:\n",
    "        features['prev-tag'] = '<start>'\n",
    "    else:\n",
    "        if pos > 1:\n",
    "            features['prev-tag'] = tagged[pos - 1][1]\n",
    "            features['prev-bigram'] = \" \".join(w.lower() for w, t in tagged[pos - 2: pos])\n",
    "            \n",
    "        features['prev-word'] = tagged[pos - 1][0].lower()\n",
    "        features['prev-tag'] = tagged[pos - 1][1]\n",
    "        \n",
    "    return features\n",
    "\n",
    "# # Lets not split the dataset here for now\n",
    "# feature_list = create_feature_list_long(extract_features_v0, descriptions, labels, cve_ids)\n",
    "\n",
    "# classifier = nltk.NaiveBayesClassifier.train(feature_list)\n",
    "# classifier.show_most_informative_features()\n",
    "\n",
    "# eval_accuracy(classifier, extract_features_v0, descriptions, labels, cve_ids, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_features_v1(tagged: list, pos: int, cve_id=None):\n",
    "    \"\"\"Extract contextual features from the sentence w.r.t given position of a word.\"\"\"\n",
    "    word, tag = tagged[pos]\n",
    "    cases = [w.isupper() for w in word]\n",
    "    features = {\n",
    "        'tag': tag,\n",
    "        'has-uppercase': any(cases) and not all(cases),\n",
    "        'vend_prod_match': similar(word, cves[cve_id].get_cpe(cpe_type='a')),\n",
    "        'word-len-threshold': len(word) > 3\n",
    "    }\n",
    "    if pos == 0:\n",
    "        features['prev-tag'] = '<start>'\n",
    "    else:\n",
    "        if pos > 1:\n",
    "            features['prev-tag'] = tagged[pos - 1][1]\n",
    "            features['prev-bigram'] = \" \".join(w.lower() for w, t in tagged[pos - 2: pos])\n",
    "            \n",
    "        features['prev-word'] = tagged[pos - 1][0].lower()\n",
    "        features['prev-tag'] = tagged[pos - 1][1]\n",
    "        \n",
    "    return features\n",
    "\n",
    "# # Lets not split the dataset here for now\n",
    "# feature_list = create_feature_list_long(extract_features_v1, descriptions, labels, cve_ids)\n",
    "\n",
    "# classifier = nltk.NaiveBayesClassifier.train(feature_list)\n",
    "# classifier.show_most_informative_features()\n",
    "\n",
    "# eval_accuracy(classifier, extract_features_v1, descriptions, labels, cve_ids, n=2)\n",
    "# eval_accuracy_spec(classifier, extract_features_v1, descriptions, labels, cve_ids, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "regex_tokenizer = nltk.RegexpTokenizer(pattern=u\"[-_]\", gaps=True)\n",
    "\n",
    "_version_pattern = u\"(\\d[.]?)+[-_]?(\\w)*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def extract_features_v2(tagged: list, pos: int, cve_id=None):\n",
    "    \"\"\"Extract contextual features from the sentence w.r.t given position of a word.\"\"\"\n",
    "    word, tag = tagged[pos]\n",
    "    word, tag = tagged[pos]\n",
    "    # retag if necessary\n",
    "    if re.match(_version_pattern, word) and tag != 'NUM':\n",
    "        tag = 'NUM'\n",
    "        tagged[pos] = word, tag\n",
    "        \n",
    "    features = {\n",
    "        'tag': tag,\n",
    "        'word-len': len(word) > 3,\n",
    "        'has-uppercase': any(w.isupper() for w in word),\n",
    "        'vend_prod_match': similar(word, cves[cve_id].get_cpe(cpe_type='a')),\n",
    "    }\n",
    "    if pos == 0:\n",
    "        features['prev-tag'] = '<start>'\n",
    "    else:\n",
    "        if pos > 1:\n",
    "            features['prev-bigram'] = \" \".join(w.lower() for w, t in tagged[pos - 2: pos])\n",
    "            \n",
    "        if pos < len(tagged):    \n",
    "            features['next-bigram'] = \" \".join(w.lower() for w, t in tagged[pos + 1: pos + 3])\n",
    "            features['next-bigram-tags'] = \" \".join(t for w, t in tagged[pos + 1: pos + 3])\n",
    "            \n",
    "        features['prev-tag'] = tagged[pos - 1][1]\n",
    "        features['prev-word'] = tagged[pos - 1][0].lower()\n",
    "        \n",
    "        \n",
    "    return features\n",
    "\n",
    "# # Lets not split the dataset here for now\n",
    "# feature_list = create_feature_list_long(extract_features_v2, descriptions, labels, cve_ids)\n",
    "\n",
    "# classifier = nltk.NaiveBayesClassifier.train(feature_list)\n",
    "# classifier.show_most_informative_features()\n",
    "\n",
    "# eval_accuracy(classifier, extract_features_v2, descriptions, labels, cve_ids, n=1)\n",
    "# eval_accuracy_spec(classifier, extract_features_v2, descriptions, labels, cve_ids, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_features_v3(tagged: list, pos: int, cve_id=None):\n",
    "    \"\"\"Extract contextual features from the sentence w.r.t given position of a word.\"\"\"\n",
    "    word, tag = tagged[pos]\n",
    "    # retag if necessary\n",
    "    if re.match(_version_pattern, word) and tag != 'NUM':\n",
    "        tag = 'NUM'\n",
    "        tagged[pos] = (word, tag)\n",
    "    \n",
    "    try:\n",
    "        ver_pos = [pos for pos, (w, t) in enumerate(tagged[pos:]) if tag == 'NUM'][0]\n",
    "    except:\n",
    "        ver_pos = None\n",
    "        \n",
    "    try:\n",
    "        ver_follows = any([pos for pos, (w, t) in enumerate(tagged[pos:]) if tag == 'NUM'])\n",
    "    except:\n",
    "        print(tagged[pos:])\n",
    "        \n",
    "    features = {\n",
    "        'tag': tag,\n",
    "        'word-len': len(word) > 3,\n",
    "        'vend_prod_match': similar(word, cves[cve_id].get_cpe(cpe_type='a')),\n",
    "        'has-uppercase': any(w.isupper() for w in word),\n",
    "        'alnum': word.isalnum(),\n",
    "        'version_pos': ver_pos,\n",
    "        'ver_follows': ver_follows,\n",
    "    }\n",
    "    if pos == 0:\n",
    "        features['prev-tag'] = '<start>'\n",
    "    else:\n",
    "        if pos > 1:\n",
    "            features['prev-tag'] = tagged[pos - 1][1]\n",
    "            features['prev-bigram'] = \" \".join(w.lower() for w, t in tagged[pos - 2: pos])\n",
    "            \n",
    "        if pos < len(tagged) - 1:    \n",
    "            features['next-bigram'] = \" \".join(w.lower() for w, t in tagged[pos + 1: pos + 3])\n",
    "            features['next-bigram-tags'] = \" \".join(t for w, t in tagged[pos + 1: pos + 3])\n",
    "            \n",
    "        features['prev-word'] = tagged[pos - 1][0]\n",
    "            \n",
    "            \n",
    "    return features\n",
    "\n",
    "# # Lets not split the dataset here for now\n",
    "# feature_list = create_feature_list_long(extract_features_v3, descriptions, labels, cve_ids)\n",
    "\n",
    "# classifier = nltk.NaiveBayesClassifier.train(feature_list)\n",
    "# classifier.show_most_informative_features()\n",
    "\n",
    "# eval_accuracy(classifier, extract_features_v3, descriptions, labels, cve_ids, n=2)\n",
    "# # eval_accuracy_spec(classifier, extract_features_v3, descriptions, labels, cve_ids, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": true,
        "row": 25,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "#### Current approach utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def get_first_sentence(description):\n",
    "    \"\"\"Get only the first sentence from the description.\"\"\"\n",
    "    sentences = nltk.sent_tokenize(description)\n",
    "    return sentences[0] if sentences else ''\n",
    "\n",
    "\n",
    "def guess_package_name(description):\n",
    "    from nltk.corpus import stopwords\n",
    "    \"\"\"Guess package name from given description.\n",
    "\n",
    "    Very naive approach. Words starting with uppercase letter\n",
    "    are considered to be possible package names (minus stop words).\n",
    "\n",
    "    Returns a list of possible package names, without duplicates.\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = set()\n",
    "\n",
    "    try:\n",
    "        # Fails when no downloaded stopwords are available.\n",
    "        stop_words.update(stopwords.words('english'))\n",
    "    except LookupError:\n",
    "        # Download stopwords since they are not available.\n",
    "        nltk.download('stopwords')\n",
    "        stop_words.update(stopwords.words('english'))\n",
    "\n",
    "    regexp = re.compile('[A-Z][A-Za-z0-9-:]*')  # ? TODO: tweak\n",
    "    suspects = regexp.findall(description)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    if not suspects:\n",
    "        return results\n",
    "\n",
    "    results = [x.lower() for x in suspects if x.lower() not in stop_words]\n",
    "    # get rid of duplicates, but keep order\n",
    "    results = list(OrderedDict.fromkeys(results))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_package_name_candidates(description):\n",
    "    \"\"\"Try to identify possible package names in the CVE's description.\"\"\"\n",
    "    pkg_name_candidates = set()\n",
    "    first_sentence = get_first_sentence(description)\n",
    "    names = guess_package_name(first_sentence)\n",
    "    pkg_name_candidates.update(set(names))\n",
    "    return pkg_name_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def eval_old_accuracy(sentences: list, labels: list) -> float:\n",
    "    from collections import Counter\n",
    "    \n",
    "    assert len(sentences) == len(labels)\n",
    "    \n",
    "    guesses = [None] * len(labels)\n",
    "    accurates = [None] * len(labels)\n",
    "    for i, sent in enumerate(sentences):\n",
    "        \n",
    "        names = get_package_name_candidates(sent)\n",
    "        guesses[i] = len(names)\n",
    "        accurates[i] = False\n",
    "        for name in names:\n",
    "            if labels[i].lower().find(name) != -1:\n",
    "                accurates[i] = True\n",
    "    \n",
    "#         print('Expected: `%s`' % labels[i], 'got: `%s`' % most_prob)\n",
    "    \n",
    "    # TODO: come up with more sophisticated way of measuring accuracy\n",
    "    bag = Counter(accurates)\n",
    "    return bag[True] / len(labels), sum(guesses) / len(guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_nof_guesses(sentences: list, labels: list) -> float:\n",
    "    assert len(sentences) == len(labels)\n",
    "    \n",
    "    guesses = [None] * len(labels)\n",
    "    for i, sent in enumerate(sentences):\n",
    "        \n",
    "        names = get_package_name_candidates(sent)\n",
    "        guesses[i] = len(names)\n",
    "        \n",
    "    # TODO: come up with more sophisticated way of measuring accuracy\n",
    "    return guesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 36,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "#### Train and test set split\n",
    "\n",
    "Toy data set will be split by $0.2$ ratio in order to be able to test model's ability to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "descriptions, labels, cve_ids = list(zip(*toy_df.sample(frac=1)[['description', 'label', 'cve_id']].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(1, 15)\n",
    "feature_extractors = [extract_features_vanilla, extract_features_v0, extract_features_v1, extract_features_v2,\n",
    "                      extract_features_v3\n",
    "                     ]\n",
    "\n",
    "split = int(len(descriptions) * 0.2)\n",
    "test_set, test_labels = descriptions[:split], labels[:split]\n",
    "train_set, train_labels = descriptions[split:], labels[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 40,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### Evaluate classificators accuracy on the test set using raw output evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 3,
        "hidden": false,
        "row": 44,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "#### Evaluation of the current used approach on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 47,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "old_accuracy, mean_guess = eval_old_accuracy(test_set, test_labels)\n",
    "print('accuracy:', old_accuracy)\n",
    "print('mean guess length:', mean_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 51,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "#### Evaluation of the new approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "accuracy_list = list()\n",
    "for i, extractor in enumerate(feature_extractors):\n",
    "    # Lets not split the dataset here for now\n",
    "    feature_list = create_feature_list_long(extractor, train_set, train_labels, cve_ids[split:])\n",
    "\n",
    "    classifier = nltk.NaiveBayesClassifier.train(feature_list)\n",
    "#     classifier.show_most_informative_features()\n",
    "    \n",
    "    accuracy_list.append([eval_accuracy(classifier, extractor, test_set, test_labels, cve_ids[:split], i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "trace_names = ['vanilla_extractor'] + ['extract_features_v%d' % i for i in range(len(accuracy_list))]\n",
    "data = [go.Scatter(x=x, y=ac, name=trace_names[i]) for i, ac in enumerate(accuracy_list)]\n",
    "\n",
    "layout = go.Layout(\n",
    "    yaxis=dict(\n",
    "        title='Accuracy',\n",
    "        titlefont=dict(\n",
    "            color='grey'\n",
    "        )\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title='Candidates',\n",
    "        titlefont=dict(\n",
    "            color='grey'\n",
    "        )\n",
    "    ),\n",
    "    shapes=[\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'x0': mean_guess,\n",
    "            'x1': mean_guess,\n",
    "            'y0': -0.05,\n",
    "            'y1': 1.1,\n",
    "            'opacity': 0.2,\n",
    "            'line': {\n",
    "                'dash': 'dash'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'x0': -0.5,\n",
    "            'x1': 20,\n",
    "            'y0': old_accuracy,\n",
    "            'y1': old_accuracy,\n",
    "            'opacity': 0.2,\n",
    "            'line': {\n",
    "                'dash': 'dash'\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 19,
        "hidden": false,
        "row": 55,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "guess_trace = go.Scatter(y=get_nof_guesses(test_set, test_labels))\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(\n",
    "        ticks='',\n",
    "        showticklabels=False,\n",
    "        showgrid=False\n",
    "    ),\n",
    "    shapes=[\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'y0': mean_guess,\n",
    "            'y1': mean_guess,\n",
    "            'x0': -0.1,\n",
    "            'x1': 23,\n",
    "            'opacity': 0.2,\n",
    "            'line': {\n",
    "                'dash': 'dash'\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[guess_trace], layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 19,
        "hidden": true,
        "row": 74,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 3,
        "hidden": false,
        "row": 74,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### Evaluate accuracy on the test set using predective evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "accuracy_list = list()\n",
    "for i, extractor in enumerate(feature_extractors):\n",
    "    # Lets not split the dataset here for now\n",
    "    feature_list = create_feature_list_long(extractor, train_set, train_labels, cve_ids[split:])\n",
    "\n",
    "    classifier = nltk.NaiveBayesClassifier.train(feature_list)\n",
    "#     classifier.show_most_informative_features()\n",
    "    \n",
    "    accuracy_list.append([eval_accuracy_spec(classifier, extractor, test_set, test_labels, cve_ids[:split], i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "trace_names = ['vanilla_extractor'] + ['extract_features_v%d' % i for i in range(len(accuracy_list))]\n",
    "data = [go.Scatter(x=x, y=ac, name=trace_names[i]) for i, ac in enumerate(accuracy_list)]\n",
    "\n",
    "layout = go.Layout(\n",
    "    yaxis=dict(\n",
    "        title='Accuracy',\n",
    "        titlefont=dict(\n",
    "            color='grey'\n",
    "        )\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title='Candidates',\n",
    "        titlefont=dict(\n",
    "            color='grey'\n",
    "        )\n",
    "    ),\n",
    "    shapes=[\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'x0': mean_guess,\n",
    "            'x1': mean_guess,\n",
    "            'y0': -0.05,\n",
    "            'y1': 1.1,\n",
    "            'opacity': 0.2,\n",
    "            'line': {\n",
    "                'dash': 'dash'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'x0': -0.5,\n",
    "            'x1': 20,\n",
    "            'y0': old_accuracy,\n",
    "            'y1': old_accuracy,\n",
    "            'opacity': 0.2,\n",
    "            'line': {\n",
    "                'dash': 'dash'\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 19,
        "hidden": false,
        "row": 77,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 3,
        "hidden": false,
        "row": 96,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### Evaluate accuracy on the toy set using predective evaluation\n",
    "\n",
    "Lets try to evaluate the accuracy of chosen feature extractors on the whole toy set using the same classifiers. No re-training being done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 2,
        "hidden": false,
        "row": 99,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "#### Evaluation of the currently used approach on the toy set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 101,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "old_accuracy, mean_guess = eval_old_accuracy(descriptions, labels)\n",
    "print('old accuracy:', old_accuracy)\n",
    "print('mean guess length:', mean_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 2,
        "hidden": false,
        "row": 105,
        "width": 12
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "#### Evaluation of the new approach on the toy set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "accuracy_list = list()\n",
    "for i, extractor in enumerate(feature_extractors):\n",
    "    # Lets not split the dataset here for now\n",
    "    feature_list = create_feature_list_long(extractor, train_set, train_labels, cve_ids[split:])\n",
    "\n",
    "    classifier = nltk.NaiveBayesClassifier.train(feature_list)\n",
    "#     classifier.show_most_informative_features()\n",
    "    \n",
    "    accuracy_list.append([eval_accuracy_spec(classifier, extractor, descriptions, labels, cve_ids, i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "trace_names = ['vanilla_extractor'] + ['extract_features_v%d' % i for i in range(len(accuracy_list))]\n",
    "data = [go.Scatter(x=x, y=ac, name=trace_names[i]) for i, ac in enumerate(accuracy_list)]\n",
    "\n",
    "layout = go.Layout(\n",
    "    yaxis=dict(\n",
    "        title='Accuracy',\n",
    "        titlefont=dict(\n",
    "            color='grey'\n",
    "        )\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title='Candidates',\n",
    "        titlefont=dict(\n",
    "            color='grey'\n",
    "        )\n",
    "    ),\n",
    "    shapes=[\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'x0': mean_guess,\n",
    "            'x1': mean_guess,\n",
    "            'y0': -0.05,\n",
    "            'y1': 1.1,\n",
    "            'opacity': 0.2,\n",
    "            'line': {\n",
    "                'dash': 'dash'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'x0': -0.5,\n",
    "            'x1': 20,\n",
    "            'y0': old_accuracy,\n",
    "            'y1': old_accuracy,\n",
    "            'opacity': 0.2,\n",
    "            'line': {\n",
    "                'dash': 'dash'\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 19,
        "hidden": false,
        "row": 107,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create trendline\n",
    "from collections import namedtuple\n",
    "\n",
    "Poly = namedtuple('Trendline', 'coefs residuals rank singular_val rcond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# get number of guesses for the old project predictions\n",
    "guesses = np.array(get_nof_guesses(descriptions, labels))\n",
    "\n",
    "# fit guesses with polynomial\n",
    "poly = Poly(*np.polyfit(np.arange(len(guesses)), guesses, 20, full=True))\n",
    "pts = np.linspace(start=0, stop=len(guesses), num=len(guesses) * 10)  # create evaluation points\n",
    "\n",
    "# create polynomial function\n",
    "f = np.poly1d(poly.coefs)\n",
    "\n",
    "x_plot = np.linspace(start=0, stop=len(guesses), num=200)\n",
    "y_plot = f(x_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# create traces\n",
    "guess_trace = go.Scatter(y=guesses, name='Project guesses', hoverinfo='name + y')\n",
    "trendline_trace = go.Scatter(x=x_plot, y=y_plot, name='Polynomial trendline',\n",
    "                            hoverinfo='skip')\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"Number of projects predicted by current approach\",\n",
    "    shapes=[\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'y0': mean_guess,\n",
    "            'y1': mean_guess,\n",
    "            'x0': -0.1,\n",
    "            'x1': 120,\n",
    "            'opacity': 0.2,\n",
    "            'line': {\n",
    "                'dash': 'dash'\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[guess_trace, trendline_trace], layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 19,
        "hidden": false,
        "row": 126,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 145,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### Evaluate accuracy on the whole dataset for 3  main ecosystems\n",
    "\n",
    "This evaluation will take a while, the whole dataset presented at the beginning of this notebook will be labeled and split into train and test data.\n",
    "\n",
    "The classifier will be retrained on the train data.\n",
    "\n",
    "**NOTE:** *It would be possible to use the same classifiers as previously, but they were trained on such a small portion of data (ever smaller with respect to the real-world data), that the results would not correspond with reality.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 11,
        "hidden": false,
        "row": 149,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Choose an extractor and train it on the whole df\n",
    "extractor = extract_features_v3\n",
    "\n",
    "feature_list = create_feature_list_long(extractor, descriptions, labels, cve_ids)\n",
    "classifier = nltk.NaiveBayesClassifier.train(feature_list)\n",
    "\n",
    "predictions = [None] * len(df)\n",
    "for i, desc in enumerate(df.description.values):\n",
    "    probs = predict(desc, classifier, extract_features_v3, cve_id=df.cve_id.values[i], n=3,\n",
    "                   verbose=False)\n",
    "    predictions[i] = probs\n",
    "\n",
    "# get just the names\n",
    "\n",
    "pred_proj_names = [tuple(zip(*p))[0] for p in predictions]\n",
    "\n",
    "pred_df = pd.Series(pred_proj_names, name='prediction')\n",
    "df['prediction'] = pred_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# label_tuple are a position of the project token in the token list\n",
    "pred_tuple = [None] * len(df)\n",
    "for i, (index, row) in enumerate(df.iterrows()):\n",
    "    proj = index[1].lower()\n",
    "    desc = row.description.lower()\n",
    "    # find the position of proj in the description, if applicable\n",
    "    tokens = nltk.word_tokenize(desc)\n",
    "    found = False\n",
    "    for pos, token in enumerate(tokens):\n",
    "        if token == proj:\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        pos = None\n",
    "    \n",
    "    pred_tuple[i] = (row.cve_id, token if found else None, pos)\n",
    "\n",
    "# turn index into series\n",
    "label_series = pd.DataFrame(pred_tuple, columns=['cve_id', 'label', 'pos'])\n",
    "\n",
    "del pred_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Label the dataset\n",
    "df = df.reset_index().merge(label_series, how='left', on='cve_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "descriptions, labels, cve_ids = list(zip(*df.sample(frac=1)[['description', 'project', 'cve_id']].values))\n",
    "\n",
    "# estimate split ratio\n",
    "split = int(len(descriptions) * 0.25)\n",
    "\n",
    "# prepare train data\n",
    "train_descriptions, train_labels, train_cve_ids = descriptions[split:], labels[split:], cve_ids[split:]\n",
    "\n",
    "# prepare test data\n",
    "test_descriptions, test_labels, test_cve_ids = descriptions[:split], labels[:split], cve_ids[:split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 160,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "#### Evaluation of the currently used approach on the whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "old_accuracy, mean_guess = eval_old_accuracy(test_descriptions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 164,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "print('accuracy:', old_accuracy)\n",
    "print('mean guess length:', mean_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 168,
        "width": 12
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "source": [
    "#### Evaluation of the new approach on the whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(1, 15)\n",
    "feature_extractors = [extract_features_vanilla, extract_features_v0, extract_features_v1, extract_features_v2,\n",
    "                      extract_features_v3\n",
    "                     ]\n",
    "\n",
    "accuracy_list = list()\n",
    "for i, extractor in enumerate(feature_extractors):\n",
    "    # Lets not split the dataset here for now\n",
    "    feature_list = create_feature_list_long(extractor, train_descriptions, train_labels,\n",
    "                                            train_cve_ids)\n",
    "\n",
    "    classifier = nltk.NaiveBayesClassifier.train(feature_list)\n",
    "#     classifier.show_most_informative_features()\n",
    "    \n",
    "    accuracy_list.append(\n",
    "        [\n",
    "            eval_accuracy_spec(classifier, extractor, test_descriptions, test_labels, test_cve_ids, i)\n",
    "            for i in x\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "trace_names = ['vanilla_extractor'] + ['extract_features_v%d' % i for i in range(len(accuracy_list))]\n",
    "data = [go.Scatter(x=x, y=ac, name=trace_names[i]) for i, ac in enumerate(accuracy_list)]\n",
    "\n",
    "layout = go.Layout(\n",
    "    yaxis=dict(\n",
    "        title='Accuracy',\n",
    "        titlefont=dict(\n",
    "            color='grey'\n",
    "        )\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title='Candidates',\n",
    "        titlefont=dict(\n",
    "            color='grey'\n",
    "        )\n",
    "    ),\n",
    "    shapes=[\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'x0': mean_guess,\n",
    "            'x1': mean_guess,\n",
    "            'y0': -0.05,\n",
    "            'y1': 1.1,\n",
    "            'opacity': 0.2,\n",
    "            'line': {\n",
    "                'dash': 'dash'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'x0': -0.5,\n",
    "            'x1': 20,\n",
    "            'y0': old_accuracy,\n",
    "            'y1': old_accuracy,\n",
    "            'opacity': 0.2,\n",
    "            'line': {\n",
    "                'dash': 'dash'\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 19,
        "hidden": false,
        "row": 172,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# get number of guesses for the old project predictions\n",
    "guesses = np.array(get_nof_guesses(test_descriptions, test_labels))\n",
    "\n",
    "# fit guesses with polynomial\n",
    "poly = Poly(*np.polyfit(np.arange(len(guesses)), guesses, 20, full=True))\n",
    "pts = np.linspace(start=0, stop=len(guesses), num=len(guesses) * 10)  # create evaluation points\n",
    "\n",
    "# create polynomial function\n",
    "f = np.poly1d(poly.coefs)\n",
    "\n",
    "x_plot = np.linspace(start=0, stop=len(guesses), num=1000)\n",
    "y_plot = f(x_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# create traces\n",
    "guess_trace = go.Scatter(y=guesses, name='Project guesses', hoverinfo='name + y')\n",
    "trendline_trace = go.Scatter(x=x_plot, y=y_plot, name='Polynomial trendline',\n",
    "                             hoverinfo='skip')\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"Number of projects predicted by current approach\",\n",
    "    shapes=[\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'y0': mean_guess,\n",
    "            'y1': mean_guess,\n",
    "            'x0': -0.1,\n",
    "            'x1': len(guesses) + 10,\n",
    "            'opacity': 0.2,\n",
    "            'line': {\n",
    "                'dash': 'dash'\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[guess_trace, trendline_trace], layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 19,
        "hidden": false,
        "row": 191,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "iplot(fig, show_link=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 3,
        "hidden": false,
        "row": 210,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "#### Incorrect predictions for the whole dataset\n",
    "\n",
    "Discarding project descriptions where the label was not present at all, get the number of incorrect predictions w.r.t the project name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "predictions = [None] * len(df)\n",
    "for i, desc in enumerate(df.description.values):\n",
    "    probs = predict(desc, classifier, extract_features_v3, cve_id=df.cve_id.values[i], n=3,\n",
    "                   verbose=False)\n",
    "    predictions[i] = probs\n",
    "\n",
    "# get just the names\n",
    "\n",
    "pred_proj_names = [tuple(zip(*p))[0] for p in predictions]\n",
    "\n",
    "pred_df = pd.Series(pred_proj_names, name='prediction')\n",
    "df['prediction'] = pred_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# label_tuple are a position of the project token in the token list\n",
    "pred_tuple = [None] * len(df)\n",
    "for i, row in df.iterrows():\n",
    "    proj = row.project.lower()\n",
    "    \n",
    "    correct = None if row.label is None else row.label in row.prediction\n",
    "    pred_tuple[i] = (row.cve_id, correct)\n",
    "\n",
    "# turn index into series\n",
    "label_series = pd.DataFrame(pred_tuple, columns=['cve_id', 'correct'])\n",
    "\n",
    "del pred_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "df_pred = df.merge(label_series, how='inner', on='cve_id').set_index(['username', 'project'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_incorrect = df_pred[(df_pred.label.notnull()) & (df_pred.correct_y == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 2,
        "hidden": false,
        "row": 213,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "print('Number of incorrect predictions: %d out of %d' % (len(df_incorrect), len(df_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 12,
        "hidden": false,
        "row": 215,
        "width": 12
       },
       "report_default": {}
      }
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_incorrect[['description', 'prediction']].reset_index().style"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
